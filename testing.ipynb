{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 knife, 253.8ms\n",
      "Speed: 18.3ms preprocess, 253.8ms inference, 14.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 knife, 276.6ms\n",
      "Speed: 13.0ms preprocess, 276.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Error loading image: knife/image3.jpg\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@15773.993] global loadsave.cpp:241 findDecoder imread_('knife/image3.jpg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 knife, 3 persons, 216.5ms\n",
      "Speed: 2.1ms preprocess, 216.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 320x640 1 celurit, 299.9ms\n",
      "Speed: 4.5ms preprocess, 299.9ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Error loading image: knife/image6.jpg\n",
      "Error loading image: knife/image7.jpg\n",
      "Error loading image: knife/image8.jpg\n",
      "Error loading image: knife/image9.jpg\n",
      "Error loading image: knife/image10.jpg\n",
      "Processing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@15799.988] global loadsave.cpp:241 findDecoder imread_('knife/image6.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15799.990] global loadsave.cpp:241 findDecoder imread_('knife/image7.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15799.992] global loadsave.cpp:241 findDecoder imread_('knife/image8.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15799.993] global loadsave.cpp:241 findDecoder imread_('knife/image9.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@15799.993] global loadsave.cpp:241 findDecoder imread_('knife/image10.jpg'): can't open/read file: check file path/integrity\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from config import YOLO_MODEL_PATH, YOLO_CLASSES, CONFIDENCE_THRESHOLD\n",
    "\n",
    "\n",
    "model = YOLO(YOLO_MODEL_PATH)\n",
    "\n",
    "def detect_objects(frame):\n",
    "    detections = []\n",
    "    results = model(frame)\n",
    "    for detection in results[0].boxes:\n",
    "        cls_id = int(detection.cls[0])\n",
    "        label = YOLO_CLASSES[cls_id]\n",
    "        conf = float(detection.conf[0])\n",
    "        if conf > CONFIDENCE_THRESHOLD:\n",
    "            x1, y1, x2, y2 = map(int, detection.xyxy[0])\n",
    "            detections.append({\n",
    "                \"label\": label,\n",
    "                \"confidence\": conf,\n",
    "                \"box\": (x1, y1, x2, y2)\n",
    "            })\n",
    "    return detections\n",
    "\n",
    "# Path to the folder containing the 10 test images\n",
    "image_folder = \"knife/\"\n",
    "image_paths = [f\"{image_folder}image{i+1}.jpg\" for i in range(10)]\n",
    "\n",
    "# Process each image\n",
    "for idx, image_path in enumerate(image_paths):\n",
    "    # Load the image\n",
    "    frame = cv2.imread(image_path)\n",
    "    if frame is None:\n",
    "        print(f\"Error loading image: {image_path}\")\n",
    "        continue\n",
    "    \n",
    "    # Detect objects\n",
    "    detections = detect_objects(frame)\n",
    "    \n",
    "    # Draw bounding boxes and labels on the image\n",
    "    for detection in detections:\n",
    "        label = detection['label']\n",
    "        confidence = detection['confidence']\n",
    "        x1, y1, x2, y2 = detection['box']\n",
    "        \n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        \n",
    "        # Prepare label text with confidence\n",
    "        label_text = f\"{label} ({confidence:.2f})\"\n",
    "        \n",
    "        # Put label text above the bounding box\n",
    "        cv2.putText(frame, label_text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    \n",
    "    # Display the image with bounding boxes\n",
    "    cv2.imshow(f\"Detections for Image {idx + 1}\", frame)\n",
    "    cv2.waitKey(0)  # Press any key to proceed to the next image\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotated video saved to output_video_with_annotations.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.lite.python.interpreter import Interpreter\n",
    "from config import TFLITE_MODEL_PATH\n",
    "\n",
    "# Load and initialize the TFLite model\n",
    "interpreter = Interpreter(model_path=TFLITE_MODEL_PATH)\n",
    "interpreter.allocate_tensors()\n",
    "runner = interpreter.get_signature_runner()\n",
    "\n",
    "# Initialize states for the TFLite model\n",
    "init_states = {\n",
    "    name: tf.zeros(x['shape'], dtype=x['dtype']).numpy()\n",
    "    for name, x in runner.get_input_details().items()\n",
    "}\n",
    "del init_states['image']\n",
    "\n",
    "def detect_violence(frame, states):\n",
    "    processed_frame = tf.image.convert_image_dtype(frame, tf.float32)\n",
    "    processed_frame = tf.image.resize_with_pad(processed_frame, 172, 172)\n",
    "    clip = tf.expand_dims(processed_frame, axis=0)\n",
    "    \n",
    "    outputs = runner(**states, image=clip)\n",
    "    logits = outputs.pop('logits')[0]\n",
    "    states = {key: value for key, value in outputs.items()}\n",
    "    \n",
    "    # Calculate softmax probabilities\n",
    "    probs = tf.nn.softmax(logits).numpy()\n",
    "    return probs, states\n",
    "\n",
    "# Path to the input and output video\n",
    "input_video_path = \"/Users/bintangrestubawono/Downloads/Equal Rights, Equal Left! When Guys Fight Back Compilation 2024 - TLS - Fighting Spirit (720p, h264, youtube).mp4\"  # Replace with your video path\n",
    "output_video_path = \"output_video_with_annotations.mp4\"\n",
    "\n",
    "# Open the input video\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Define the codec and create a VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "# Start processing the video\n",
    "states = init_states\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert BGR (OpenCV format) to RGB for TensorFlow\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Run violence detection\n",
    "    probs, states = detect_violence(rgb_frame, states)\n",
    "    fight_prob, no_fight_prob = probs[0] * 100, probs[1] * 100\n",
    "\n",
    "    # Overlay the probabilities on the frame\n",
    "    overlay_text = f\"Violence: {fight_prob:.2f}%  |  No Violence: {no_fight_prob:.2f}%\"\n",
    "    cv2.putText(frame, overlay_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Write the frame with annotation to the output video\n",
    "    out.write(frame)\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "print(f\"Annotated video saved to {output_video_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.74\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "NOT VIOLENCE       0.67      0.95      0.79       200\n",
      "    VIOLENCE       0.92      0.53      0.67       200\n",
      "\n",
      "    accuracy                           0.74       400\n",
      "   macro avg       0.79      0.74      0.73       400\n",
      "weighted avg       0.79      0.74      0.73       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.lite.python.interpreter import Interpreter\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from config import TFLITE_MODEL_PATH\n",
    "\n",
    "# Load and initialize the TFLite model\n",
    "interpreter = Interpreter(model_path=TFLITE_MODEL_PATH)\n",
    "interpreter.allocate_tensors()\n",
    "runner = interpreter.get_signature_runner()\n",
    "\n",
    "# Initialize states for the TFLite model\n",
    "init_states = {\n",
    "    name: tf.zeros(x['shape'], dtype=x['dtype']).numpy()\n",
    "    for name, x in runner.get_input_details().items()\n",
    "}\n",
    "del init_states['image']\n",
    "\n",
    "def detect_violence(frame, states):\n",
    "    processed_frame = tf.image.convert_image_dtype(frame, tf.float32)\n",
    "    processed_frame = tf.image.resize_with_pad(processed_frame, 172, 172)\n",
    "    clip = tf.expand_dims(processed_frame, axis=0)\n",
    "    \n",
    "    outputs = runner(**states, image=clip)\n",
    "    logits = outputs.pop('logits')[0]\n",
    "    states = {key: value for key, value in outputs.items()}\n",
    "    \n",
    "    # Calculate softmax probabilities\n",
    "    probs = tf.nn.softmax(logits).numpy()\n",
    "    return probs, states\n",
    "\n",
    "# Path to the dataset\n",
    "dataset_path = \"/Users/bintangrestubawono/Documents/Violence Detection/datasets/RWF-2000/val/\"  # Folder containing subfolders \"Fight\" and \"NoFight\"\n",
    "categories = [\"No_Fight\", \"Fight\"]\n",
    "\n",
    "# Initialize storage for predictions and true labels\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "for category in categories:\n",
    "    category_path = os.path.join(dataset_path, category)\n",
    "    label = 1 if category == \"Fight\" else 0  # 1 for Fight, 0 for NoFight\n",
    "\n",
    "    for video_file in os.listdir(category_path):\n",
    "        video_path = os.path.join(category_path, video_file)\n",
    "        \n",
    "        # Open the video file\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        states = init_states\n",
    "        frame_predictions = []\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Convert BGR (OpenCV format) to RGB for TensorFlow\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Run violence detection\n",
    "            probs, states = detect_violence(rgb_frame, states)\n",
    "            fight_prob = probs[0]  # Probability of violence\n",
    "\n",
    "            # Append frame-level prediction (1 for Violence if >0.8, else 0 for No Violence)\n",
    "            frame_predictions.append(1 if fight_prob > 0.6 else 0)\n",
    "\n",
    "        # Release video resources\n",
    "        cap.release()\n",
    "\n",
    "        # Determine video-level prediction by majority voting\n",
    "        if len(frame_predictions) > 0:\n",
    "            video_prediction = 1 if sum(frame_predictions) > (len(frame_predictions) / 2) else 0\n",
    "        else:\n",
    "            video_prediction = 0  # Default to NoFight if no frames were processed\n",
    "\n",
    "        # Append video-level prediction and true label\n",
    "        all_predictions.append(video_prediction)\n",
    "        all_true_labels.append(label)\n",
    "\n",
    "# Calculate accuracy and classification report\n",
    "accuracy = accuracy_score(all_true_labels, all_predictions)\n",
    "report = classification_report(all_true_labels, all_predictions, target_names=[\"NOT VIOLENCE\", \"VIOLENCE\"])\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 11 persons, 239.3ms\n",
      "Speed: 12.1ms preprocess, 239.3ms inference, 15.2ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from config import YOLO_MODEL_PATH, YOLO_CLASSES, CONFIDENCE_THRESHOLD\n",
    "\n",
    "# Load YOLO model\n",
    "model = YOLO(YOLO_MODEL_PATH)\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    x1_inter = max(box1[0], box2[0])\n",
    "    y1_inter = max(box1[1], box2[1])\n",
    "    x2_inter = min(box1[2], box2[2])\n",
    "    y2_inter = min(box1[3], box2[3])\n",
    "\n",
    "    intersection_area = max(0, x2_inter - x1_inter) * max(0, y2_inter - y1_inter)\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union_area = box1_area + box2_area - intersection_area\n",
    "\n",
    "    return intersection_area / union_area if union_area != 0 else 0\n",
    "\n",
    "def non_max_suppression(detections, iou_threshold):\n",
    "    detections = sorted(detections, key=lambda x: x[\"confidence\"], reverse=True)\n",
    "    suppressed_boxes = []\n",
    "    while detections:\n",
    "        chosen_box = detections.pop(0)\n",
    "        suppressed_boxes.append(chosen_box)\n",
    "        detections = [box for box in detections if calculate_iou(chosen_box[\"box\"], box[\"box\"]) < iou_threshold]\n",
    "    return suppressed_boxes\n",
    "\n",
    "def detect_objects(frame):\n",
    "    detections = []\n",
    "    results = model(frame)\n",
    "    for detection in results[0].boxes:\n",
    "        cls_id = int(detection.cls[0])\n",
    "        label = YOLO_CLASSES[cls_id]\n",
    "        conf = float(detection.conf[0])\n",
    "        if conf > CONFIDENCE_THRESHOLD:\n",
    "            x1, y1, x2, y2 = map(int, detection.xyxy[0])\n",
    "            detections.append({\n",
    "                \"label\": label,\n",
    "                \"confidence\": conf,\n",
    "                \"box\": (x1, y1, x2, y2)\n",
    "            })\n",
    "    return detections\n",
    "\n",
    "def main(image_path):\n",
    "    # Load image\n",
    "    frame = cv2.imread(image_path)\n",
    "    \n",
    "    # Detect objects\n",
    "    detections = detect_objects(frame)\n",
    "    \n",
    "    # Filter detections for \"People\"\n",
    "    filtered_detections = [d for d in detections if d['label'] == 'person']\n",
    "    \n",
    "    # Apply Non-Max Suppression\n",
    "    iou_threshold = 0.5  # Adjust this threshold as needed\n",
    "    suppressed_detections = non_max_suppression(filtered_detections, iou_threshold)\n",
    "    \n",
    "    # Draw boxes and labels on the image\n",
    "    for det in suppressed_detections:\n",
    "        x1, y1, x2, y2 = det['box']\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)  # Draw rectangle\n",
    "        cv2.putText(frame, f\"{det['label']} {det['confidence']:.2f}\", (x1, y1 - 5),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "    \n",
    "    # Display the result\n",
    "    cv2.imshow(\"Detections\", frame)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Specify your image path\n",
    "image_path = \"/Users/bintangrestubawono/Downloads/istockphoto-1457298134-612x612.jpg\"  # Replace with your image path\n",
    "main(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 11 persons, 247.5ms\n",
      "Speed: 15.9ms preprocess, 247.5ms inference, 11.3ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from config import YOLO_MODEL_PATH, YOLO_CLASSES, CONFIDENCE_THRESHOLD\n",
    "\n",
    "# Load YOLO model\n",
    "model = YOLO(YOLO_MODEL_PATH)\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    x1_inter = max(box1[0], box2[0])\n",
    "    y1_inter = max(box1[1], box2[1])\n",
    "    x2_inter = min(box1[2], box2[2])\n",
    "    y2_inter = min(box1[3], box2[3])\n",
    "\n",
    "    intersection_area = max(0, x2_inter - x1_inter) * max(0, y2_inter - y1_inter)\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union_area = box1_area + box2_area - intersection_area\n",
    "\n",
    "    return intersection_area / union_area if union_area != 0 else 0\n",
    "\n",
    "def non_max_suppression(detections, iou_threshold):\n",
    "    detections = sorted(detections, key=lambda x: x[\"confidence\"], reverse=True)\n",
    "    suppressed_boxes = []\n",
    "    \n",
    "    while detections:\n",
    "        chosen_box = detections.pop(0)\n",
    "        suppressed_boxes.append(chosen_box)\n",
    "        \n",
    "        detections = [box for box in detections if calculate_iou(chosen_box[\"box\"], box[\"box\"]) < iou_threshold]\n",
    "    \n",
    "    return suppressed_boxes\n",
    "\n",
    "def detect_objects(frame):\n",
    "    detections = []\n",
    "    results = model(frame)\n",
    "    for detection in results[0].boxes:\n",
    "        cls_id = int(detection.cls[0])\n",
    "        label = YOLO_CLASSES[cls_id]\n",
    "        conf = float(detection.conf[0])\n",
    "        if conf > CONFIDENCE_THRESHOLD:\n",
    "            x1, y1, x2, y2 = map(int, detection.xyxy[0])\n",
    "            detections.append({\n",
    "                \"label\": label,\n",
    "                \"confidence\": conf,\n",
    "                \"box\": (x1, y1, x2, y2)\n",
    "            })\n",
    "    return detections\n",
    "\n",
    "def main(image_path):\n",
    "    # Load image\n",
    "    frame = cv2.imread(image_path)\n",
    "    \n",
    "    # Detect objects\n",
    "    detections = detect_objects(frame)\n",
    "    \n",
    "    # Filter detections for \"People\"\n",
    "    filtered_detections = [d for d in detections if d['label'] == 'person']\n",
    "    \n",
    "    # Apply Non-Max Suppression\n",
    "    iou_threshold = 0.5  # Adjust this threshold as needed\n",
    "    suppressed_detections = non_max_suppression(filtered_detections, iou_threshold)\n",
    "    \n",
    "    # Label groups\n",
    "    group_detections = []\n",
    "    for det in suppressed_detections:\n",
    "        group_detections.append({\n",
    "            \"label\": \"group\",  # Labeling as 'group'\n",
    "            \"confidence\": det['confidence'],\n",
    "            \"box\": det['box']\n",
    "        })\n",
    "\n",
    "    # Draw boxes and labels on the image\n",
    "    for det in group_detections:\n",
    "        x1, y1, x2, y2 = det['box']\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)  # Draw rectangle\n",
    "        cv2.putText(frame, f\"{det['label']} {det['confidence']:.2f}\", (x1, y1 - 5),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "    \n",
    "    # Display the result\n",
    "    cv2.imshow(\"Detections\", frame)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Specify your image path\n",
    "image_path = \"/Users/bintangrestubawono/Downloads/istockphoto-1457298134-612x612.jpg\"  # Replace with your image path\n",
    "main(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 11 persons, 225.1ms\n",
      "Speed: 1.5ms preprocess, 225.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from config import YOLO_MODEL_PATH, YOLO_CLASSES, CONFIDENCE_THRESHOLD\n",
    "\n",
    "# Load YOLO model\n",
    "model = YOLO(YOLO_MODEL_PATH)\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    x1_inter = max(box1[0], box2[0])\n",
    "    y1_inter = max(box1[1], box2[1])\n",
    "    x2_inter = min(box1[2], box2[2])\n",
    "    y2_inter = min(box1[3], box2[3])\n",
    "\n",
    "    intersection_area = max(0, x2_inter - x1_inter) * max(0, y2_inter - y1_inter)\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union_area = box1_area + box2_area - intersection_area\n",
    "\n",
    "    return intersection_area / union_area if union_area != 0 else 0\n",
    "\n",
    "def group_overlapping_boxes(detections, iou_threshold):\n",
    "    grouped_boxes = []\n",
    "    while detections:\n",
    "        base_box = detections.pop(0)\n",
    "        group = [base_box]\n",
    "        \n",
    "        # Group all detections that have IoU >= threshold with the base box\n",
    "        i = 0\n",
    "        while i < len(detections):\n",
    "            if calculate_iou(base_box[\"box\"], detections[i][\"box\"]) >= iou_threshold:\n",
    "                group.append(detections.pop(i))\n",
    "            else:\n",
    "                i += 1\n",
    "                \n",
    "        # Determine if this is a group or a single person\n",
    "        if len(group) > 1:\n",
    "            # Calculate the bounding box that covers the entire group\n",
    "            x1 = min(box[\"box\"][0] for box in group)\n",
    "            y1 = min(box[\"box\"][1] for box in group)\n",
    "            x2 = max(box[\"box\"][2] for box in group)\n",
    "            y2 = max(box[\"box\"][3] for box in group)\n",
    "            confidence = max(box[\"confidence\"] for box in group)\n",
    "            \n",
    "            grouped_boxes.append({\n",
    "                \"label\": \"group\",\n",
    "                \"confidence\": confidence,\n",
    "                \"box\": (x1, y1, x2, y2)\n",
    "            })\n",
    "        else:\n",
    "            # Single person detection, keep original box and label\n",
    "            grouped_boxes.append({\n",
    "                \"label\": \"person\",\n",
    "                \"confidence\": base_box[\"confidence\"],\n",
    "                \"box\": base_box[\"box\"]\n",
    "            })\n",
    "        \n",
    "    return grouped_boxes\n",
    "\n",
    "def detect_objects(frame):\n",
    "    detections = []\n",
    "    results = model(frame)\n",
    "    for detection in results[0].boxes:\n",
    "        cls_id = int(detection.cls[0])\n",
    "        label = YOLO_CLASSES[cls_id]\n",
    "        conf = float(detection.conf[0])\n",
    "        if conf > CONFIDENCE_THRESHOLD:\n",
    "            x1, y1, x2, y2 = map(int, detection.xyxy[0])\n",
    "            detections.append({\n",
    "                \"label\": label,\n",
    "                \"confidence\": conf,\n",
    "                \"box\": (x1, y1, x2, y2)\n",
    "            })\n",
    "    return detections\n",
    "\n",
    "def main(image_path):\n",
    "    # Load image\n",
    "    frame = cv2.imread(image_path)\n",
    "    \n",
    "    # Detect objects\n",
    "    detections = detect_objects(frame)\n",
    "    \n",
    "    # Filter detections for \"People\"\n",
    "    filtered_detections = [d for d in detections if d['label'] == 'person']\n",
    "    \n",
    "    # Group overlapping bounding boxes\n",
    "    iou_threshold = 0.2  # Threshold to consider boxes as overlapping\n",
    "    grouped_detections = group_overlapping_boxes(filtered_detections, iou_threshold)\n",
    "    \n",
    "    # Draw boxes and labels on the image\n",
    "    for det in grouped_detections:\n",
    "        x1, y1, x2, y2 = det['box']\n",
    "        color = (0, 255, 0) if det['label'] == 'person' else (255, 0, 0)  # Green for person, Blue for group\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)  # Draw rectangle\n",
    "        cv2.putText(frame, f\"{det['label']} {det['confidence']:.2f}\", (x1, y1 - 5),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    \n",
    "    # Display the result\n",
    "    cv2.imshow(\"Detections\", frame)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Specify your image path\n",
    "image_path = \"/Users/bintangrestubawono/Downloads/istockphoto-1457298134-612x612.jpg\"  # Replace with your image path\n",
    "main(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution - (/Users/bintangrestubawono/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/bintangrestubawono/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: SpeechRecognition in /Users/bintangrestubawono/anaconda3/lib/python3.10/site-packages (3.8.1)\n",
      "Collecting pydub\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "\u001b[33mWARNING: Error parsing dependencies of textract: .* suffix can only be used with `==` or `!=` operators\n",
      "    extract-msg (<=0.29.*)\n",
      "                 ~~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Users/bintangrestubawono/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/bintangrestubawono/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: pydub\n",
      "\u001b[33mWARNING: Ignoring invalid distribution - (/Users/bintangrestubawono/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/bintangrestubawono/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed pydub-0.25.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install SpeechRecognition pydub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution - (/Users/bintangrestubawono/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/bintangrestubawono/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: ffmpeg-python in /Users/bintangrestubawono/anaconda3/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: future in /Users/bintangrestubawono/anaconda3/lib/python3.10/site-packages (from ffmpeg-python) (0.18.3)\n",
      "\u001b[33mWARNING: Error parsing dependencies of textract: .* suffix can only be used with `==` or `!=` operators\n",
      "    extract-msg (<=0.29.*)\n",
      "                 ~~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Users/bintangrestubawono/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/bintangrestubawono/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Users/bintangrestubawono/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/bintangrestubawono/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ffmpeg-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution - (/Users/bintangrestubawono/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/bintangrestubawono/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting aiortc\n",
      "  Downloading aiortc-1.9.0-cp38-abi3-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: opencv-python in /Users/bintangrestubawono/anaconda3/lib/python3.10/site-packages (4.7.0.72)\n",
      "Collecting aioice<1.0.0,>=0.9.0 (from aiortc)\n",
      "  Downloading aioice-0.9.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: av<13.0.0,>=9.0.0 in /Users/bintangrestubawono/anaconda3/lib/python3.10/site-packages (from aiortc) (10.0.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /Users/bintangrestubawono/anaconda3/lib/python3.10/site-packages (from aiortc) (1.15.1)\n",
      "Requirement already satisfied: cryptography>=42.0.0 in /Users/bintangrestubawono/anaconda3/lib/python3.10/site-packages (from aiortc) (42.0.8)\n",
      "Collecting google-crc32c>=1.1 (from aiortc)\n",
      "  Downloading google_crc32c-1.6.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (2.3 kB)\n",
      "Collecting pyee>=9.0.0 (from aiortc)\n",
      "  Downloading pyee-12.0.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting pylibsrtp>=0.10.0 (from aiortc)\n",
      "  Downloading pylibsrtp-0.10.0-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting pyopenssl>=24.0.0 (from aiortc)\n",
      "  Downloading pyOpenSSL-24.2.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /Users/bintangrestubawono/anaconda3/lib/python3.10/site-packages (from opencv-python) (1.26.4)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /Users/bintangrestubawono/anaconda3/lib/python3.10/site-packages (from aioice<1.0.0,>=0.9.0->aiortc) (2.6.1)\n",
      "Collecting ifaddr>=0.2.0 (from aioice<1.0.0,>=0.9.0->aiortc)\n",
      "  Downloading ifaddr-0.2.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: pycparser in /Users/bintangrestubawono/anaconda3/lib/python3.10/site-packages (from cffi>=1.0.0->aiortc) (2.21)\n",
      "Requirement already satisfied: typing-extensions in /Users/bintangrestubawono/anaconda3/lib/python3.10/site-packages (from pyee>=9.0.0->aiortc) (4.9.0)\n",
      "Downloading aiortc-1.9.0-cp38-abi3-macosx_11_0_arm64.whl (896 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m896.0/896.0 kB\u001b[0m \u001b[31m916.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aioice-0.9.0-py3-none-any.whl (24 kB)\n",
      "Downloading google_crc32c-1.6.0-cp310-cp310-macosx_12_0_arm64.whl (30 kB)\n",
      "Downloading pyee-12.0.0-py3-none-any.whl (14 kB)\n",
      "Downloading pylibsrtp-0.10.0-cp38-abi3-macosx_11_0_arm64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m799.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyOpenSSL-24.2.1-py3-none-any.whl (58 kB)\n",
      "Downloading ifaddr-0.2.0-py3-none-any.whl (12 kB)\n",
      "\u001b[33mWARNING: Error parsing dependencies of textract: .* suffix can only be used with `==` or `!=` operators\n",
      "    extract-msg (<=0.29.*)\n",
      "                 ~~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/Users/bintangrestubawono/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/bintangrestubawono/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: ifaddr, pyee, google-crc32c, aioice, pylibsrtp, pyopenssl, aiortc\n",
      "  Attempting uninstall: pyopenssl\n",
      "    Found existing installation: pyOpenSSL 23.2.0\n",
      "    Uninstalling pyOpenSSL-23.2.0:\n",
      "      Successfully uninstalled pyOpenSSL-23.2.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution - (/Users/bintangrestubawono/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Users/bintangrestubawono/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "nevermined-sdk-py 0.14.0 requires contracts-lib-py==1.0.0, but you have contracts-lib-py 1.0.4 which is incompatible.\n",
      "nevermined-sdk-py 0.14.0 requires web3==5.26.0, but you have web3 5.31.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aioice-0.9.0 aiortc-1.9.0 google-crc32c-1.6.0 ifaddr-0.2.0 pyee-12.0.0 pylibsrtp-0.10.0 pyopenssl-24.2.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install aiortc opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 109\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m pc\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 109\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/asyncio/runners.py:33\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m coroutines\u001b[38;5;241m.\u001b[39miscoroutine(main):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma coroutine was expected, got \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(main))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import cv2\n",
    "import numpy as np\n",
    "from aiortc import RTCPeerConnection, RTCSessionDescription, MediaStreamTrack\n",
    "from aiortc.contrib.signaling import TcpSocketSignaling\n",
    "from av import VideoFrame\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class VideoReceiver:\n",
    "    def __init__(self):\n",
    "        self.track = None\n",
    "\n",
    "    async def handle_track(self, track):\n",
    "        print(\"Inside handle track\")\n",
    "        self.track = track\n",
    "        frame_count = 0\n",
    "        while True:\n",
    "            try:\n",
    "                print(\"Waiting for frame...\")\n",
    "                frame = await asyncio.wait_for(track.recv(), timeout=5.0)\n",
    "                frame_count += 1\n",
    "                print(f\"Received frame {frame_count}\")\n",
    "                \n",
    "                if isinstance(frame, VideoFrame):\n",
    "                    print(f\"Frame type: VideoFrame, pts: {frame.pts}, time_base: {frame.time_base}\")\n",
    "                    frame = frame.to_ndarray(format=\"bgr24\")\n",
    "                elif isinstance(frame, np.ndarray):\n",
    "                    print(f\"Frame type: numpy array\")\n",
    "                else:\n",
    "                    print(f\"Unexpected frame type: {type(frame)}\")\n",
    "                    continue\n",
    "              \n",
    "                 # Add timestamp to the frame\n",
    "                current_time = datetime.now()\n",
    "                new_time = current_time - timedelta( seconds=55)\n",
    "                timestamp = new_time.strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "                cv2.putText(frame, timestamp, (10, frame.shape[0] - 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "                cv2.imwrite(f\"imgs/received_frame_{frame_count}.jpg\", frame)\n",
    "                print(f\"Saved frame {frame_count} to file\")\n",
    "                cv2.imshow(\"Frame\", frame)\n",
    "    \n",
    "                # Exit on 'q' key press\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "            except asyncio.TimeoutError:\n",
    "                print(\"Timeout waiting for frame, continuing...\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in handle_track: {str(e)}\")\n",
    "                if \"Connection\" in str(e):\n",
    "                    break\n",
    "        print(\"Exiting handle_track\")\n",
    "async def run(pc, signaling):\n",
    "    await signaling.connect()\n",
    "\n",
    "    @pc.on(\"track\")\n",
    "    def on_track(track):\n",
    "        if isinstance(track, MediaStreamTrack):\n",
    "            print(f\"Receiving {track.kind} track\")\n",
    "            asyncio.ensure_future(video_receiver.handle_track(track))\n",
    "\n",
    "    @pc.on(\"datachannel\")\n",
    "    def on_datachannel(channel):\n",
    "        print(f\"Data channel established: {channel.label}\")\n",
    "\n",
    "    @pc.on(\"connectionstatechange\")\n",
    "    async def on_connectionstatechange():\n",
    "        print(f\"Connection state is {pc.connectionState}\")\n",
    "        if pc.connectionState == \"connected\":\n",
    "            print(\"WebRTC connection established successfully\")\n",
    "\n",
    "    print(\"Waiting for offer from sender...\")\n",
    "    offer = await signaling.receive()\n",
    "    print(\"Offer received\")\n",
    "    await pc.setRemoteDescription(offer)\n",
    "    print(\"Remote description set\")\n",
    "\n",
    "    answer = await pc.createAnswer()\n",
    "    print(\"Answer created\")\n",
    "    await pc.setLocalDescription(answer)\n",
    "    print(\"Local description set\")\n",
    "\n",
    "    await signaling.send(pc.localDescription)\n",
    "    print(\"Answer sent to sender\")\n",
    "\n",
    "    print(\"Waiting for connection to be established...\")\n",
    "    while pc.connectionState != \"connected\":\n",
    "        await asyncio.sleep(0.1)\n",
    "\n",
    "    print(\"Connection established, waiting for frames...\")\n",
    "    await asyncio.sleep(100)  # Wait for 35 seconds to receive frames\n",
    "\n",
    "    print(\"Closing connection\")\n",
    "\n",
    "async def main():\n",
    "    signaling = TcpSocketSignaling(\"https://7319-202-145-7-5.ngrok-free.app/\", 3000)\n",
    "    pc = RTCPeerConnection()\n",
    "    \n",
    "    global video_receiver\n",
    "    video_receiver = VideoReceiver()\n",
    "\n",
    "    try:\n",
    "        await run(pc, signaling)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main: {str(e)}\")\n",
    "    finally:\n",
    "        print(\"Closing peer connection\")\n",
    "        await pc.close()\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
